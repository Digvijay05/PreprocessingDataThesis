{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-11T10:17:44.058125Z",
     "iopub.status.busy": "2026-02-11T10:17:44.057286Z",
     "iopub.status.idle": "2026-02-11T10:17:44.337881Z",
     "shell.execute_reply": "2026-02-11T10:17:44.337048Z",
     "shell.execute_reply.started": "2026-02-11T10:17:44.058097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:17:44.339900Z",
     "iopub.status.busy": "2026-02-11T10:17:44.339442Z",
     "iopub.status.idle": "2026-02-11T10:17:50.303560Z",
     "shell.execute_reply": "2026-02-11T10:17:50.302625Z",
     "shell.execute_reply.started": "2026-02-11T10:17:44.339875Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "ðŸ“¦ Pandas version: 3.0.0\n",
      "ðŸ“¦ NumPy version: 2.4.2\n",
      "ðŸ“Š Dataset: 100,000 Insurance Records | 54+ Features\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "COLORS = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E']\n",
    "\n",
    "# Machine Learning - Regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, \n",
    "                             mean_absolute_percentage_error)\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ“¦ Pandas version: {pd.__version__}\")\n",
    "print(f\"ðŸ“¦ NumPy version: {np.__version__}\")\n",
    "print(f\"ðŸ“Š Dataset: 100,000 Insurance Records | 54+ Features\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:17:50.305316Z",
     "iopub.status.busy": "2026-02-11T10:17:50.304389Z",
     "iopub.status.idle": "2026-02-11T10:17:50.958318Z",
     "shell.execute_reply": "2026-02-11T10:17:50.957495Z",
     "shell.execute_reply.started": "2026-02-11T10:17:50.305289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"medical_insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:17:50.960245Z",
     "iopub.status.busy": "2026-02-11T10:17:50.959672Z",
     "iopub.status.idle": "2026-02-11T10:17:50.972238Z",
     "shell.execute_reply": "2026-02-11T10:17:50.971547Z",
     "shell.execute_reply.started": "2026-02-11T10:17:50.960218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:17:50.974230Z",
     "iopub.status.busy": "2026-02-11T10:17:50.973935Z",
     "iopub.status.idle": "2026-02-11T10:17:51.002166Z",
     "shell.execute_reply": "2026-02-11T10:17:51.001489Z",
     "shell.execute_reply.started": "2026-02-11T10:17:50.974210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Drop leakage-prone columns\n",
    "drop_cols = [\"claims_count\", \"avg_claim_amount\", \"total_claims_paid\", \"is_high_risk\", \"had_major_procedure\"]\n",
    "data.drop(columns=drop_cols, errors=\"ignore\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:17:51.003092Z",
     "iopub.status.busy": "2026-02-11T10:17:51.002865Z",
     "iopub.status.idle": "2026-02-11T10:17:51.514493Z",
     "shell.execute_reply": "2026-02-11T10:17:51.513798Z",
     "shell.execute_reply.started": "2026-02-11T10:17:51.003049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ Possible Leakage Features: ['annual_medical_cost', 'monthly_premium', 'annual_premium']\n",
      "Train=80000, Val=10000, Test=10000\n"
     ]
    }
   ],
   "source": [
    "# 3.1 --- LEAKAGE AUDIT ---\n",
    "target = 'annual_medical_cost'\n",
    "corr = data.corr(numeric_only=True)[target].abs().sort_values(ascending=False)\n",
    "\n",
    "# Flag suspicious correlations > 0.85\n",
    "suspect_features = corr[corr > 0.85].index.tolist()\n",
    "print(\"ðŸš¨ Possible Leakage Features:\", suspect_features)\n",
    "\n",
    "# Optional: drop those if domain knowledge confirms post-t0\n",
    "data = data.drop(columns=[f for f in suspect_features if f != target], errors='ignore')\n",
    "\n",
    "# 3.2 --- PROVENANCE TAGGING ---\n",
    "provenance = {}\n",
    "for col in data.columns:\n",
    "    if col in ['person_id', 'sex_num', 'education_num', 'region_num']:\n",
    "        provenance[col] = 'static'\n",
    "    elif any(k in col for k in ['visit','hosp','medication','premium','deductible','copay','risk_proxy']):\n",
    "        provenance[col] = 'pre_t0_dynamic'\n",
    "    else:\n",
    "        provenance[col] = 'uncertain'\n",
    "\n",
    "provenance_data = pd.DataFrame.from_dict(provenance, orient='index', columns=['provenance'])\n",
    "\n",
    "# 3.3 --- DATA SPLITTING STRATEGY ---\n",
    "\n",
    "# If person_id repeats across rows -> group split; else simple train/test\n",
    "if data['person_id'].duplicated().any():\n",
    "    splitter = GroupKFold(n_splits=5)\n",
    "    for fold, (train_idx, val_idx) in enumerate(splitter.split(data, data[target], groups=data['person_id'])):\n",
    "        print(f\"Fold {fold}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "        break  # preview one fold\n",
    "else:\n",
    "    # Single occurrence per person -> random split\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "    print(f\"Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
    "\n",
    "# 3.4 --- PERMUTATION SANITY CHECK (optional quick)\n",
    "# Baseline performance with shuffled target to ensure no leakage signals\n",
    "# (to be done after model is built)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:17:51.515375Z",
     "iopub.status.busy": "2026-02-11T10:17:51.515172Z",
     "iopub.status.idle": "2026-02-11T10:17:51.521841Z",
     "shell.execute_reply": "2026-02-11T10:17:51.520931Z",
     "shell.execute_reply.started": "2026-02-11T10:17:51.515350Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['person_id', 'age', 'sex', 'region', 'urban_rural', 'income',\n",
       "       'education', 'marital_status', 'employment_status', 'household_size',\n",
       "       'dependents', 'bmi', 'smoker', 'alcohol_freq', 'visits_last_year',\n",
       "       'hospitalizations_last_3yrs', 'days_hospitalized_last_3yrs',\n",
       "       'medication_count', 'systolic_bp', 'diastolic_bp', 'ldl', 'hba1c',\n",
       "       'plan_type', 'network_tier', 'deductible', 'copay', 'policy_term_years',\n",
       "       'policy_changes_last_2yrs', 'provider_quality', 'risk_score',\n",
       "       'annual_medical_cost', 'chronic_count', 'hypertension', 'diabetes',\n",
       "       'asthma', 'copd', 'cardiovascular_disease', 'cancer_history',\n",
       "       'kidney_disease', 'liver_disease', 'arthritis', 'mental_health',\n",
       "       'proc_imaging_count', 'proc_surgery_count', 'proc_physio_count',\n",
       "       'proc_consult_count', 'proc_lab_count'],\n",
       "      dtype='str')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-11T10:21:50.645744Z",
     "iopub.status.busy": "2026-02-11T10:21:50.645147Z",
     "iopub.status.idle": "2026-02-11T10:22:22.558192Z",
     "shell.execute_reply": "2026-02-11T10:22:22.557523Z",
     "shell.execute_reply.started": "2026-02-11T10:21:50.645724Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- PHASE 4: MODEL CREATION (Baseline Models + Preprocessing Pipeline) ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Copy the engineered dataset\n",
    "data_model = data.copy()\n",
    "\n",
    "# Drop redundant and target columns\n",
    "data_model = data_model.drop(columns=['monthly_premium'], errors='ignore')\n",
    "target = 'annual_medical_cost'\n",
    "\n",
    "# Define features and target\n",
    "X = data_model.drop(columns=[target, 'person_id'], errors='ignore')\n",
    "y = data_model[target]\n",
    "\n",
    "# Split into train, validation, test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = [col for col in X_train.columns if col not in numeric_features]\n",
    "\n",
    "# --- PREPROCESSING PIPELINE ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- BASELINE MODELS ---\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"XGBRegressor\":XGBRegressor(),\n",
    "    \"RandomForestRegressor\":RandomForestRegressor()\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_val)\n",
    "    \n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    r2 = r2_score(y_val, preds)\n",
    "    \n",
    "    results.append({\"Model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>1756.881537</td>\n",
       "      <td>2773.010482</td>\n",
       "      <td>0.179087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>1756.886623</td>\n",
       "      <td>2773.019412</td>\n",
       "      <td>0.179082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>1757.087883</td>\n",
       "      <td>2773.373759</td>\n",
       "      <td>0.178872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>1806.650335</td>\n",
       "      <td>2875.574222</td>\n",
       "      <td>0.117239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>1865.157870</td>\n",
       "      <td>2861.928268</td>\n",
       "      <td>0.125597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model          MAE         RMSE        R2\n",
       "0       LinearRegression  1756.881537  2773.010482  0.179087\n",
       "1                  Ridge  1756.886623  2773.019412  0.179082\n",
       "2                  Lasso  1757.087883  2773.373759  0.178872\n",
       "3           XGBRegressor  1806.650335  2875.574222  0.117239\n",
       "4  RandomForestRegressor  1865.157870  2861.928268  0.125597"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8447134,
     "sourceId": 13324273,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
